import requests
import urllib3
from bs4 import BeautifulSoup
import sys, os
import re
from playwright.async_api import async_playwright

# è·¯å¾‘è¨­å®šï¼šå„ªå…ˆè¼‰å…¥ä¸Šå±¤æ¨¡çµ„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# é—œé–‰ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å¸¸ç”¨æ­£å‰‡é›†ä¸­åŒ–
DATE_RE = re.compile(r"\d{4}[./]\d{2}[./]\d{2}")
CAPTION_RE = re.compile(r"ï¼ˆ[^ï¼‰]*(?:æ”|æä¾›)[^ï¼‰]*ï¼‰$")

# å„æ–°èä¾†æºçš„éæ¿¾å­—å…¸
EXCLUDE_KEYWORDS = {
    "ctinews": ["æ¨™ç±¤","ç•™è¨€","è¿½è¹¤æˆ‘å€‘","æ–°èåˆ†é¡","å½±éŸ³å°ˆå€","é—œæ–¼æˆ‘å€‘","å®¢æœè³‡è¨Š","è¯çµ¡æˆ‘å€‘","ç‰ˆæ¬Š","China Times Group"],
    "knews": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","ç‰ˆæ¬Š","å®¢æœ","è¿½è¹¤","æ¨è–¦æ–°è","ä¸‹è¼‰","App","â—åŠ å…¥"],
    "ebc": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","ç‰ˆæ¬Š","æ›´å¤šæ–°è","App","ä¸‹è¼‰","å„ªæƒ ","æŠ˜æ‰£","æ»¿é¡","å“ç‰Œ","æ´»å‹•"],
    "ctwant": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","æ›´å¤šç²¾å½©å…§å®¹","ç‰ˆæ¬Š","å®¢æœ","è¿½è¹¤","ä¸‹è¼‰","App","ç«‹å³è¨‚é–±","ç²¾å½©å½±éŸ³","åœ–ï¼","è«‹ç”¨å¾®ä¿¡æƒæ","æƒæ QR Code","æ›´å¤š CTWANT å ±å°","å®‰è£æˆ‘å€‘çš„ CTWANT APP","ä¸‹ä¸€å‰‡æ–°è","äººæ°£æ–°è","é—œéµç†±æœ","éš±ç§æ¬Šæ”¿ç­–","Â©","iPhoneç«‹å³å®‰è£","Androidç«‹å³å®‰è£"],
    "setn": ["ä¿è­·è¢«å®³äººéš±ç§","æ‹’çµ•å®¶åº­æš´åŠ›","è«‹æ’¥æ‰“110","è«‹æ’¥æ‰“113","å½°åŒ–å¤«å¦»","æ´»æ˜¥å®®","æ›´å¤šæ–°è","å»¶ä¼¸é–±è®€","ç‰ˆæ¬Šæ‰€æœ‰","ä¸‰ç«‹æ–°èç¶²"],
    "ettoday": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","æ›´å¤šæ–°è","ç‰ˆæ¬Šæ‰€æœ‰","ETtodayæ–°èé›²","è«‹ç”¨å¾®ä¿¡æƒæ","æƒæ QR Code","å®‰è£æˆ‘å€‘çš„ APP","ç²¾å½©å½±éŸ³","éš±ç§æ¬Šæ”¿ç­–","Â©","iPhoneç«‹å³å®‰è£","Androidç«‹å³å®‰è£","â–²"],
    "udn": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","æ›´å¤šæ–°è","ç‰ˆæ¬Šæ‰€æœ‰","è¯åˆæ–°èç¶²","éš±ç§æ¬Šæ”¿ç­–","Â©","Appä¸‹è¼‰","ç«‹å³è¨‚é–±","ç²¾å½©å½±éŸ³","æœ¬å ±è³‡æ–™ç…§ç‰‡"],
    "chinatimes": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","æ›´å¤šæ–°è","ç‰ˆæ¬Šæ‰€æœ‰","ä¸­æ™‚æ–°èç¶²","éš±ç§æ¬Šæ”¿ç­–","Â©","Appä¸‹è¼‰","ç«‹å³è¨‚é–±","ç²¾å½©å½±éŸ³"],
    "mirrordaily": ["çŒœä½ å–œæ­¡","å…¶ä»–äººéƒ½åœ¨çœ‹","ç›¸é—œæ–°è","å»¶ä¼¸é–±è®€","æ¨è–¦","æ›´å¤š","è¿½è¹¤","åˆ†äº«","ç‰ˆæ¬Š","éš±ç§æ¬Š","æœå‹™æ¢æ¬¾","ç•™è¨€","è¨‚é–±","App","ä¸‹è¼‰","TOP","è¿”å›","ç¤¾ç¾¤","ç†±é–€","æœ€æ–°"],
    "tvbs": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","æ›´å¤šæ–°è","ç‰ˆæ¬Šæ‰€æœ‰","TVBSæ–°èç¶²","éš±ç§æ¬Šæ”¿ç­–","Â©","Appä¸‹è¼‰","ç«‹å³è¨‚é–±","ç²¾å½©å½±éŸ³","â—¤","ğŸ‘‰","å„ªæƒ ","æŠ˜æ‰£","æ»¿é¡","æ´»å‹•","æ—…éŠå„ªæƒ ","åŠ å…¥TVBSæ–°èLINE","TVBSéµç²‰","ä¸‹è¼‰APP","å…è²»æ‹¿é»æ•¸","æŠ½iPhone","eSIM","éŸ“äºèˆªç©º","è¨‚æˆ¿æœ€ä¾¿å®œ","çœéŒ¢æ”»ç•¥"],
    "mirrormedia": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","æ›´å¤šæ–°è","ç‰ˆæ¬Šæ‰€æœ‰","é¡é€±åˆŠ","éš±ç§æ¬Šæ”¿ç­–","Â©","Appä¸‹è¼‰","ç«‹å³è¨‚é–±","ç²¾å½©å½±éŸ³","ç•™è¨€","ç†±é–€æ–°è","TOP","è¿”å›","ç¤¾ç¾¤åˆ†äº«","ç¿»æ”","ç…§ç‰‡","åœ–ç‰‡","è‡‰æ›¸","Instagram"],
    "mnews": ["å»¶ä¼¸é–±è®€","ç›¸é—œæ–°è","æ›´å¤šæ–°è","ç‰ˆæ¬Šæ‰€æœ‰","é¡æ–°è","éš±ç§æ¬Šæ”¿ç­–","Â©","Appä¸‹è¼‰","ç«‹å³è¨‚é–±","ç²¾å½©å½±éŸ³","ç•™è¨€","ç†±é–€æ–°è","TOP","è¿”å›","ç¤¾ç¾¤åˆ†äº«","ç¿»æ”","ç…§ç‰‡","åœ–ç‰‡","è‡‰æ›¸","Instagram"]
}

async def fetch_content(url: str) -> str:
    try:
        # å£¹è˜‹ç¶²ï¼šPlaywrightï¼ˆasyncï¼‰
        if "nextapple.com" in url:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                page = await browser.new_page()
                await page.goto(url, timeout=30000, wait_until="domcontentloaded")
                html = await page.content()
                await page.close()
                await browser.close()

            soup = BeautifulSoup(html, "html.parser")

            title_tag = soup.find("h1")
            title = title_tag.get_text(strip=True) if title_tag else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            lead = ""
            for div in soup.find_all(["div", "h2"]):
                text = div.get_text(strip=True)
                if text.startswith("ã€è¨˜è€…") and "å ±å°" in text:
                    lead = text
                    break

            paragraphs = []
            for p in soup.find_all("p"):
                text = p.get_text(strip=True)
                if text:
                    if text.startswith("ã€è¨˜è€…") and "å ±å°" in text and paragraphs:
                        break
                    paragraphs.append(text)

            return "\n".join([title, lead] + paragraphs)

        # ä¸­å¤©ç¶²
        elif "ctinews.com" in url or "ctitv.com.tw" in url or "cti.com.tw" in url:
            resp = requests.get(url, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.find("h1")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS["ctinews"]
            paragraphs = []
            for p in soup.find_all("p"):
                text = p.get_text(strip=True)
                if text and len(text) >= 6 and not any(kw in text for kw in exclude_keywords):
                    paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # çŸ¥æ–°è
        elif "knews.com.tw" in url:
            resp = requests.get(url, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.find("h1") or soup.find("h2")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS["knews"]
            paragraphs = []
            for p in soup.find_all("p"):
                text = p.get_text(strip=True)
                if text and len(text) >= 6 and not any(kw in text for kw in exclude_keywords):
                    paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # æ±æ£®æ–°è
        elif "ebc.net.tw" in url:
            resp = requests.get(url, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.find("h1")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS["ebc"]
            paragraphs = []
            for p in soup.find_all("p"):
                text = p.get_text(strip=True)
                if text and len(text) >= 6 and not any(kw in text for kw in exclude_keywords):
                    paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # å‘¨åˆŠç‹ï¼ˆå¿…è¦æ™‚ç”¨ Playwright async é‡æŠ“ï¼‰
        elif "ctwant.com" in url:
            resp = requests.get(url, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.find("h1") or soup.find("h2")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS["ctwant"]
            paragraphs = []
            content_div = soup.select_one("div.article-content") or soup
            for p in content_div.find_all("p"):
                text = p.get_text(strip=True)
                if text and not any(kw in text for kw in exclude_keywords):
                    paragraphs.append(text)

            if len(paragraphs) < 3:
                async with async_playwright() as p:
                    browser = await p.chromium.launch()
                    page = await browser.new_page()
                    await page.goto(url, timeout=30000, wait_until="domcontentloaded")
                    html = await page.content()
                    await page.close()
                    await browser.close()

                soup = BeautifulSoup(html, "html.parser")
                paragraphs = []
                content_div = soup.select_one("div.article-content") or soup
                for p in content_div.find_all("p"):
                    text = p.get_text(strip=True)
                    if text and not any(kw in text for kw in exclude_keywords):
                        paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # ä¸‰ç«‹æ–°èç¶²
        elif "setn.com" in url:
            resp = requests.get(url, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.find("h1") or soup.find("h2")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS["setn"]
            paragraphs = []
            content_div = soup.select_one("div.NewsContent") or soup.select_one("div.Content") or soup
            for p in content_div.find_all("p"):
                text = p.get_text(strip=True)
                if text and len(text) >= 6 and not any(kw in text for kw in exclude_keywords):
                    paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # ETtodayæ–°èé›²
        elif "ettoday.net" in url:
            resp = requests.get(url, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.find("h1") or soup.find("h2")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS["ettoday"]
            paragraphs = []
            content_div = soup.select_one("div.story") or soup
            for p in content_div.find_all("p"):
                text = p.get_text(strip=True)
                if text and len(text) >= 6 and not any(kw in text for kw in exclude_keywords):
                    paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # UDN è¯åˆæ–°èç¶²ï¼šPlaywrightï¼ˆasyncï¼‰
        elif "udn.com" in url:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                page = await browser.new_page()
                await page.goto(url, timeout=30000, wait_until="domcontentloaded")
                html = await page.content()
                await page.close()
                await browser.close()

            soup = BeautifulSoup(html, "html.parser")

            title = soup.find("h1") or soup.find("h2")
            if title:
                title_text = title.get_text(strip=True)
            else:
                og_title = soup.select_one('meta[property="og:title"]')
                title_text = og_title["content"].strip() if og_title and og_title.get("content") else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            content_divs = soup.select("div.story-content, section.article-content__editor, div.article-content")
            if not content_divs:
                return title_text

            paragraphs = []
            for div in content_divs:
                for p in div.find_all("p", recursive=True):
                    if p.find_parent("figure") or p.find_parent("figcaption"):
                        continue
                    text = p.get_text(strip=True)
                    if text:
                        paragraphs.append(text)

            seen, clean_paragraphs = set(), []
            for para in paragraphs:
                if para not in seen:
                    clean_paragraphs.append(para)
                    seen.add(para)

            return "\n".join([title_text] + clean_paragraphs)

        # ä¸­æ™‚æ–°èç¶²ï¼šPlaywrightï¼ˆasyncï¼Œè‡ªè¨‚ context/headersï¼‰
        elif "chinatimes.com" in url:
            async with async_playwright() as p:
                browser = await p.chromium.launch()
                context = await browser.new_context(
                    viewport={"width": 1920, "height": 1080},
                    device_scale_factor=1,
                    is_mobile=False,
                    has_touch=False,
                    java_script_enabled=True
                )
                page = await context.new_page()
                await page.set_extra_http_headers({
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                                  "Chrome/120.0.6099.71 Safari/537.36 Edg/120.0.6099.71",
                    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
                    "Referer": "https://www.google.com/"
                })
                await page.goto(url, timeout=60000, wait_until="domcontentloaded")
                await page.wait_for_selector("div.article-body, div.article-content", timeout=10000)
                html = await page.content()
                await page.close()
                await context.close()
                await browser.close()

            soup = BeautifulSoup(html, "html.parser")
            title_text = "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"
            og_title = soup.select_one('meta[property="og:title"]')
            if og_title and og_title.get("content"):
                title_text = og_title["content"].strip()
            else:
                meta_title = soup.select_one('meta[name="title"]')
                if meta_title and meta_title.get("content"):
                    title_text = meta_title["content"].strip()

            exclude_keywords = EXCLUDE_KEYWORDS["chinatimes"]
            paragraphs = []
            content_div = soup.select_one("div.article-body") or soup.select_one("div.article-content") or soup
            for p in content_div.find_all("p"):
                text = p.get_text(strip=True)
                if text and len(text) >= 6 and not any(kw in text for kw in exclude_keywords):
                    paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # é¡å ± Mirror Daily
        elif "mirrordaily.news" in url:
            resp = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 (KHTML, like Gecko) "
                              "Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://www.google.com/",
            }, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.find("h1") or soup.find("title")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS.get("mirrordaily", [])
            paragraphs = []

            brief = soup.find("article", class_="brief story-renderer")
            if brief:
                for t in brief.stripped_strings:
                    if t.strip():
                        paragraphs.append(t.strip())

            article_body = soup.find(attrs={"itemprop": "articleBody"}) or soup.find("div", class_="articleBody")
            if article_body:
                for t in article_body.stripped_strings:
                    if t.strip():
                        paragraphs.append(t.strip())

            seen, clean_paragraphs = set(), []
            for para in paragraphs:
                if para not in seen:
                    clean_paragraphs.append(para)
                    seen.add(para)

            return "\n".join([title_text] + clean_paragraphs)

        # TVBSæ–°èç¶²
        elif "tvbs.com.tw" in url:
            resp = requests.get(url, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.select_one("h1.title") or soup.select_one("h1.news-title")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS.get("tvbs", [])
            paragraphs = []
            content_div = soup.select_one("div#news_detail_div")
            if content_div:
                for p in content_div.find_all("p"):
                    text = p.get_text(strip=True)
                    if text and len(text) > 6 and not any(kw in text for kw in exclude_keywords):
                        paragraphs.append(text)
                for node in content_div.stripped_strings:
                    text = node.strip()
                    if text and len(text) > 6 and not any(kw in text for kw in exclude_keywords):
                        if text not in paragraphs:
                            paragraphs.append(text)

            extra_divs = soup.select("div.article_content, div[align=center]")
            for div in extra_divs:
                for p in div.find_all("p"):
                    text = p.get_text(strip=True)
                    if text and len(text) > 6 and not any(kw in text for kw in exclude_keywords):
                        if text not in paragraphs:
                            paragraphs.append(text)

            return "\n".join([title_text] + paragraphs)

        # é¡é€±åˆŠ Mirror Media
        elif "mirrormedia.mg" in url:
            resp = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 (KHTML, like Gecko) "
                              "Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://www.google.com/",
            }, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.select_one("h1.story__title")
            if title:
                title_text = title.get_text(strip=True)
            else:
                og_title = soup.select_one('meta[property="og:title"]')
                title_text = og_title["content"].strip() if og_title and og_title.get("content") else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            exclude_keywords = EXCLUDE_KEYWORDS.get("mirrormedia", [])
            paragraphs = []

            brief_div = soup.select_one("div.brief__BriefContainer-sc-e5902095-0, div.brief__BriefContainer")
            if brief_div:
                for node in brief_div.stripped_strings:
                    text = node.strip()
                    if text and len(text) > 6 and not any(kw in text for kw in exclude_keywords):
                        paragraphs.append(text)

            content_sections = soup.select("section.article-content__Wrapper-sc-f590bf19-0, section.article-content__Wrapper")
            for sec in content_sections:
                for node in sec.stripped_strings:
                    text = node.strip()
                    if text and len(text) > 6 and not any(kw in text for kw in exclude_keywords):
                        paragraphs.append(text)

            seen, clean_paragraphs = set(), []
            for para in paragraphs:
                if para not in seen:
                    clean_paragraphs.append(para)
                    seen.add(para)

            return "\n".join([title_text] + clean_paragraphs)

        # é¡æ–°è mnews.tw
        elif "mnews.tw" in url:
            resp = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 (KHTML, like Gecko) "
                              "Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://www.google.com/",
            }, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.select_one("h1")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            paragraphs = []
            brief_div = soup.select_one("div.article-brief_briefWrapper__Gm_Bu")
            if brief_div:
                for node in brief_div.stripped_strings:
                    text = node.strip()
                    if text and len(text) > 6:
                        paragraphs.append(text)

            content_articles = soup.select("section.story_contentWrapper__dvkWW > article")
            for article in content_articles:
                for p in article.find_all("p"):
                    if p.find("a"):
                        continue
                    text = p.get_text(strip=True)
                    if text and len(text) >= 6 and not DATE_RE.search(text):
                        paragraphs.append(text)

            seen, clean_paragraphs = set(), []
            for para in paragraphs:
                if para not in seen:
                    clean_paragraphs.append(para)
                    seen.add(para)

            return "\n".join([title_text] + clean_paragraphs)

        # è‡ªç”±æ™‚å ± LTN
        elif "ltn.com.tw" in url:
            resp = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 (KHTML, like Gecko) "
                              "Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://www.google.com/",
            }, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.select_one("h1")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            paragraphs = []
            content_ps = soup.select("div.text p")
            for p in content_ps:
                if p.find("a"):
                    continue
                text = p.get_text(strip=True)
                if not text or len(text) < 6:
                    continue
                if DATE_RE.search(text):
                    continue
                if "æ”" in text or "æä¾›" in text:
                    continue
                paragraphs.append(text)

            seen, clean_paragraphs = set(), []
            for para in paragraphs:
                if para not in seen:
                    clean_paragraphs.append(para)
                    seen.add(para)

            return "\n".join([title_text] + clean_paragraphs)

        # ä¸­å¤®ç¤¾ CNA
        elif "cna.com.tw" in url:
            resp = requests.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 (KHTML, like Gecko) "
                              "Chrome/120.0.0.0 Safari/537.36",
                "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://www.google.com/",
            }, timeout=15, verify=False)
            resp.encoding = "utf-8"
            soup = BeautifulSoup(resp.text, "html.parser")

            title = soup.select_one("h1")
            title_text = title.get_text(strip=True) if title else "ï¼ˆæœªèƒ½æŠ“å–æ¨™é¡Œï¼‰"

            paragraphs = []
            content_ps = soup.select("div.paragraph p, div.article p")
            for p in content_ps:
                if p.find("a"):
                    continue
                text = p.get_text(strip=True)
                if not text or len(text) < 6:
                    continue
                if DATE_RE.search(text) or re.search(r"\(\d{2}/\d{2}\s+\d{2}:\d{2}\s+æ›´æ–°\)", text):
                    continue
                if text.startswith("ï¼ˆä¸­å¤®ç¤¾è¨˜è€…"):
                    paragraphs.append(text)
                    continue
                if CAPTION_RE.search(text) or ("ç¿»æ”ç…§ç‰‡" in text):
                    continue
                if "ä¸å¾—è½‰è¼‰" in text or "ç‰ˆæ¬Š" in text:
                    continue
                paragraphs.append(text)

            seen, clean_paragraphs = set(), []
            for para in paragraphs:
                if para not in seen:
                    clean_paragraphs.append(para)
                    seen.add(para)

            return "\n".join([title_text] + clean_paragraphs)

        else:
            return "ï¼ˆç›®å‰å°šæœªæ”¯æ´æ­¤ä¾†æºï¼‰"

    except requests.RequestException as e:
        return f"ï¼ˆç¶²è·¯éŒ¯èª¤: {e}ï¼‰"
    except Exception as e:
        return f"ï¼ˆæŠ“å–å¤±æ•—: {e}ï¼‰"

# âœ… æä¾›åˆ¥åï¼Œè®“ Bot å¯ä»¥ç”¨ fetch_news_content
fetch_news_content = fetch_content
